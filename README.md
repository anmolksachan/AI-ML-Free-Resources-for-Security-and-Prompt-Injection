# AI/ML Security and Prompt Injection Resources

![output](https://github.com/user-attachments/assets/ca866203-8e57-4063-9d63-4ac919ed7b07)
<!--Image genereated via https://deepai.org/machine-learning-model/text2img-->

This repository contains a curated list of free resources focused on AI/ML security, particularly on prompt injection attacks and large language model (LLM) security. These resources include articles, guides, tools, and training materials to help you understand and secure AI systems.

## Table of Contents
- [Articles & Guides](#articles--guides)
- [Interactive Tools](#interactive-tools)
- [Repositories & Projects](#repositories--projects)
- [Training & Playgrounds](#training--playgrounds)

## Articles & Guides

- **[Portswigger Web Security Academy](https://portswigger.net/web-security/llm-attacks)**  
  Comprehensive guide on LLM attacks and defenses.

- **[IBM](https://www.ibm.com/topics/prompt-injection)**  
  Insights on prompt injection attacks and mitigation strategies.

- **[Learn Prompting](https://learnprompting.org/docs/prompt_hacking/injection)**  
  A detailed resource on prompt hacking and its security implications.

- **[OWASP](https://genai.owasp.org/)**  
  OWASP's resources for AI security, including prompt injection topics.

- **[AI Village](https://aivillage.org/large%20language%20models/threat-modeling-llm/)**  
  A collection of resources and threat models for LLM security.

- **[Promptingguide](https://www.promptingguide.ai/risks/adversarial)**  
  Information on adversarial attacks in LLMs.

- **[Bugcrowd Blog](https://www.bugcrowd.com/blog/ai-vulnerability-deep-dive-prompt-injection/)**  
  In-depth exploration of AI vulnerabilities, focusing on prompt injection.

- **[Simon Willison](https://simonwillison.net/2023/May/2/prompt-injection-explained/)**  
  A blog post explaining prompt injection attacks in detail.

- **[NCC Group](https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/)**  
  Research on prompt injection attacks by NCC Group.

- **[The Ultimate Guide to Managing Ethical and Security Risks in AI](https://www.hackerone.com/resources/e-book/the-ultimate-guide-to-managing-ethical-and-security-risks-in-ai)**  
  A guide to managing the ethical and security risks in AI by HackerOne.

## Interactive Tools

- **[Gandalf](https://gandalf.lakera.ai/)**  
  Test the robustness of your LLM prompts against common attacks.

- **[Prompt Airlines](https://promptairlines.com/)**  
  A gamified experience to learn about prompt injections.

- **[Crucible](https://crucible.dreadnode.io/)**  
  Interactive scenarios for testing AI security.

- **[Immersive Labs](https://prompting.ai.immersivelabs.com/)**  
  Training platform for AI security and prompt injection.

## Repositories & Projects

- **[WithSecureLabs - Damn Vulnerable LLM Agent](https://github.com/WithSecureLabs/damn-vulnerable-llm-agent)**  
  A repository of intentionally vulnerable LLM agents for testing.

- **[ScottLogic Prompt Injection](https://github.com/ScottLogic/prompt-injection)**  
  Resources and playground for understanding prompt injection attacks.

- **[Greshake LLM Security](https://github.com/greshake/llm-security)**  
  A collection of LLM security tools and resources.

- **[Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security)**  
  An awesome list of resources related to LLM security.

- **[Hannibal046 - Awesome LLM](https://github.com/Hannibal046/Awesome-LLM)**  
  Curated resources for everything related to LLMs, including security.

- **[Ottosulin - Awesome AI Security](https://github.com/ottosulin/awesome-ai-security)**  
  A collection of AI security resources, including prompt injection.

## Training & Playgrounds

- **[Offensive ML Playbook](https://wiki.offsecml.com/Welcome+to+the+Offensive+ML+Playbook)**  
  A playbook for offensive machine learning, including testing LLMs.

- **[Prompt Injection Games by Secdim](https://play.secdim.com/game/ai)**  
  Games designed to teach prompt injection in a fun and interactive way.

- **[Large Language Model (LLM) Pentesting](https://systemweakness.com/large-language-model-llm-pen-testing-part-i-2ef96acb6763)**  
  A guide on pentesting large language models.

- **[ATLAS Matrix](https://atlas.mitre.org/matrices/ATLAS/)**  
  MITRE's ATLAS matrix for adversarial tactics and techniques in AI.

- **[SpyLogic Prompt Injection Attack Playground](https://github.com/ScottLogic/prompt-injection)**  
  A playground to experiment with prompt injection attacks.

## E-Books & PDFs

- **[Bugcrowd Ultimate Guide AI Security](https://www.bugcrowd.com/wp-content/uploads/2024/04/Ultimate-Guide-AI-Security.pdf)**  
  Comprehensive guide to AI security, including prompt injection.

- **[Lakera - Real World LLM Exploits](https://lakera-marketing-public.s3.eu-west-1.amazonaws.com/Lakera%2BAI%2B-%2BReal%2BWorld%2BLLM%2BExploits%2B(Jan%2B2024)-min.pdf)**  
  A document highlighting real-world LLM exploits.

- **[Snyk OWASP Top 10 LLM](https://go.snyk.io/rs/677-THP-415/images/owasp-top-10-llm.pdf)**  
  Snyk's OWASP Top 10 for LLM applications.

## Resources

- **[LLM PENTEST: LEVERAGING AGENT INTEGRATION FOR RCE](https://www.blazeinfosec.com/post/llm-pentest-agent-hacking/)**

- **[]()**

- **[]()**

## Contributing

If you know of any other valuable resources that should be included, feel free to submit a pull request or open an issue.
