# AI/ML Pentesting Roadmap for Beginners

![output](https://github.com/user-attachments/assets/ca866203-8e57-4063-9d63-4ac919ed7b07)

Welcome to the roadmap for learning AI/ML pentesting from scratch! This guide will take you through the essentials of AI/ML security, focusing on prompt injection attacks, LLM security, and more. Follow these steps to build a solid foundation and advance your skills.

## Table of Contents
- [Introduction to AI/ML Security](#introduction-to-aiml-security)
- [Understanding Prompt Injection](#understanding-prompt-injection)
- [Hands-On Practice](#hands-on-practice)
- [Advanced Topics](#advanced-topics)
- [Additional Resources](#additional-resources)

## Introduction to AI/ML Security

1. **Learn the Basics of AI/ML**
   - **[AI/ML Concepts](https://www.coursera.org/learn/machine-learning)**
   - **[Introduction to Machine Learning](https://www.edx.org/course/introduction-to-machine-learning)**
   - **[Basic AI/ML Security Concepts](https://www.cybrary.it/course/machine-learning-security/)**
   
2. **Understand AI/ML Security Fundamentals**
   - **[The Ultimate Guide to Managing Ethical and Security Risks in AI](https://www.hackerone.com/resources/e-book/the-ultimate-guide-to-managing-ethical-and-security-risks-in-ai)**
   - **[From MLOps to MLOops: Exposing the Attack Surface of Machine Learning Platforms](https://jfrog.com/blog/from-mlops-to-mloops-exposing-the-attack-surface-of-machine-learning-platforms/)**

## Understanding Prompt Injection

1. **Explore Prompt Injection Basics**
   - **[IBM's Guide on Prompt Injection](https://www.ibm.com/topics/prompt-injection)**
   - **[Learn Prompting - Prompt Hacking and Injection](https://learnprompting.org/docs/prompt_hacking/injection)**
   - **[Simon Willison's Explanation](https://simonwillison.net/2023/May/2/prompt-injection-explained/)**

2. **Dive into Advanced Prompt Injection Topics**
   - **[Bugcrowd's Deep Dive](https://www.bugcrowd.com/blog/ai-vulnerability-deep-dive-prompt-injection/)**
   - **[NCC Group Research on Prompt Injection](https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/)**
   - **[Prompt Injection Analysis by JFrog](https://jfrog.com/blog/prompt-injection-attack-code-execution-in-vanna-ai-cve-2024-5565/)**

## Hands-On Practice

1. **Interactive Tools and Platforms**
   - **[Gandalf - LLM Prompt Testing](https://gandalf.lakera.ai/)**
   - **[Prompt Airlines - Gamified Learning](https://promptairlines.com/)**
   - **[Crucible - Interactive Scenarios](https://crucible.dreadnode.io/)**
   - **[Immersive Labs for AI Security](https://prompting.ai.immersivelabs.com/)**

2. **Repositories & Projects**
   - **[WithSecureLabs - Damn Vulnerable LLM Agent](https://github.com/WithSecureLabs/damn-vulnerable-llm-agent)**
   - **[ScottLogic Prompt Injection Playground](https://github.com/ScottLogic/prompt-injection)**
   - **[Greshake LLM Security Tools](https://github.com/greshake/llm-security)**
   - **[Awesome LLM Security List](https://github.com/corca-ai/awesome-llm-security)**

## Advanced Topics

1. **Deepen Your Knowledge**
   - **[LLM Pentest: Agent Integration for RCE](https://www.blazeinfosec.com/post/llm-pentest-agent-hacking/)**
   - **[How to Persuade an LLM to Change Its System Prompt](https://medium.com/@KonradDaWo/how-to-persuade-a-llm-to-change-its-system-prompt-to-aid-in-ctf-challenges-e74c1d570ed3)**
   - **[CTF Writeup - LLM Edition](https://medium.com/@embossdotar/ctf-writeup-hackpack-ctf-2024-llm-edition-yellowdog-1-db02a36e1051)**

2. **Research and Real-World Exploits**
   - **[Google AI Studio Data Exfiltration](https://embracethered.com/blog/posts/2024/google-ai-studio-data-exfiltration-now-fixed/)**
   - **[Shelltorch Vulnerabilities in Torchserve](https://www.oligo.security/blog/shelltorch-explained-multiple-vulnerabilities-in-pytorch-model-server)**
   - **[My LLM Bug Bounty Journey](https://medium.com/@zpbrent/my-llm-bug-bounty-journey-on-hugging-face-hub-via-protect-ai-9f3a1bc72c2e)**

## Additional Resources

- **[LLM Hacker's Handbook](https://github.com/forcesunseen/llm-hackers-handbook?tab=readme-ov-file)**
- **[AI Exploits Repository](https://github.com/protectai/ai-exploits?tab=readme-ov-file)**
- **[Google Gemini Vulnerability Research](https://hiddenlayer.com/research/new-google-gemini-content-manipulation-vulns-found/#Overview)**
- **[OpenAI Allowed Unlimited Credit on New Accounts](https://checkmarx.com/blog/openai-allowed-unlimited-credit-on-new-accounts/)**


<!--

# AI/ML Security and Prompt Injection Resources

![output](https://github.com/user-attachments/assets/ca866203-8e57-4063-9d63-4ac919ed7b07)

This repository contains a curated list of free resources focused on AI/ML security, particularly on prompt injection attacks and large language model (LLM) security. These resources include articles, guides, tools, and training materials to help you understand and secure AI systems.

## Table of Contents
- [Articles & Guides](#articles--guides)
- [Interactive Tools](#interactive-tools)
- [Repositories & Projects](#repositories--projects)
- [Training & Playgrounds](#training--playgrounds)

## Articles & Guides

- **[Portswigger Web Security Academy](https://portswigger.net/web-security/llm-attacks)**  
  Comprehensive guide on LLM attacks and defenses.

- **[IBM](https://www.ibm.com/topics/prompt-injection)**  
  Insights on prompt injection attacks and mitigation strategies.

- **[Learn Prompting](https://learnprompting.org/docs/prompt_hacking/injection)**  
  A detailed resource on prompt hacking and its security implications.

- **[OWASP](https://genai.owasp.org/)**  
  OWASP's resources for AI security, including prompt injection topics.

- **[AI Village](https://aivillage.org/large%20language%20models/threat-modeling-llm/)**  
  A collection of resources and threat models for LLM security.

- **[Promptingguide](https://www.promptingguide.ai/risks/adversarial)**  
  Information on adversarial attacks in LLMs.

- **[Bugcrowd Blog](https://www.bugcrowd.com/blog/ai-vulnerability-deep-dive-prompt-injection/)**  
  In-depth exploration of AI vulnerabilities, focusing on prompt injection.

- **[Simon Willison](https://simonwillison.net/2023/May/2/prompt-injection-explained/)**  
  A blog post explaining prompt injection attacks in detail.

- **[NCC Group](https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/)**  
  Research on prompt injection attacks by NCC Group.

- **[The Ultimate Guide to Managing Ethical and Security Risks in AI](https://www.hackerone.com/resources/e-book/the-ultimate-guide-to-managing-ethical-and-security-risks-in-ai)**  
  A guide to managing the ethical and security risks in AI by HackerOne.

## Interactive Tools

- **[Gandalf](https://gandalf.lakera.ai/)**  
  Test the robustness of your LLM prompts against common attacks.

- **[Prompt Airlines](https://promptairlines.com/)**  
  A gamified experience to learn about prompt injections.

- **[Crucible](https://crucible.dreadnode.io/)**  
  Interactive scenarios for testing AI security.

- **[Immersive Labs](https://prompting.ai.immersivelabs.com/)**  
  Training platform for AI security and prompt injection.

## Repositories & Projects

- **[WithSecureLabs - Damn Vulnerable LLM Agent](https://github.com/WithSecureLabs/damn-vulnerable-llm-agent)**  
  A repository of intentionally vulnerable LLM agents for testing.

- **[ScottLogic Prompt Injection](https://github.com/ScottLogic/prompt-injection)**  
  Resources and playground for understanding prompt injection attacks.

- **[Greshake LLM Security](https://github.com/greshake/llm-security)**  
  A collection of LLM security tools and resources.

- **[Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security)**  
  An awesome list of resources related to LLM security.

- **[Hannibal046 - Awesome LLM](https://github.com/Hannibal046/Awesome-LLM)**  
  Curated resources for everything related to LLMs, including security.

- **[Ottosulin - Awesome AI Security](https://github.com/ottosulin/awesome-ai-security)**  
  A collection of AI security resources, including prompt injection.

## Training & Playgrounds

- **[Offensive ML Playbook](https://wiki.offsecml.com/Welcome+to+the+Offensive+ML+Playbook)**  
  A playbook for offensive machine learning, including testing LLMs.

- **[Prompt Injection Games by Secdim](https://play.secdim.com/game/ai)**  
  Games designed to teach prompt injection in a fun and interactive way.

- **[Large Language Model (LLM) Pentesting](https://systemweakness.com/large-language-model-llm-pen-testing-part-i-2ef96acb6763)**  
  A guide on pentesting large language models.

- **[ATLAS Matrix](https://atlas.mitre.org/matrices/ATLAS/)**  
  MITRE's ATLAS matrix for adversarial tactics and techniques in AI.

- **[SpyLogic Prompt Injection Attack Playground](https://github.com/ScottLogic/prompt-injection)**  
  A playground to experiment with prompt injection attacks.

## E-Books & PDFs

- **[Bugcrowd Ultimate Guide AI Security](https://www.bugcrowd.com/wp-content/uploads/2024/04/Ultimate-Guide-AI-Security.pdf)**  
  Comprehensive guide to AI security, including prompt injection.

- **[Lakera - Real World LLM Exploits](https://lakera-marketing-public.s3.eu-west-1.amazonaws.com/Lakera%2BAI%2B-%2BReal%2BWorld%2BLLM%2BExploits%2B(Jan%2B2024)-min.pdf)**  
  A document highlighting real-world LLM exploits.

- **[Snyk OWASP Top 10 LLM](https://go.snyk.io/rs/677-THP-415/images/owasp-top-10-llm.pdf)**  
  Snyk's OWASP Top 10 for LLM applications.

- **[LLM Hacker's Handbook](https://github.com/forcesunseen/llm-hackers-handbook?tab=readme-ov-file)**


## Resources

- **[LLM PENTEST: LEVERAGING AGENT INTEGRATION FOR RCE](https://www.blazeinfosec.com/post/llm-pentest-agent-hacking/)**

- **[How to persuade a LLM to change it’s system prompt to aid in CTF challenges](https://medium.com/@KonradDaWo/how-to-persuade-a-llm-to-change-its-system-prompt-to-aid-in-ctf-challenges-e74c1d570ed3)**

- **[CTF Writeup — HackPack CTF 2024 — LLM edition — YellowDog-1](https://medium.com/@embossdotar/ctf-writeup-hackpack-ctf-2024-llm-edition-yellowdog-1-db02a36e1051)**

- **[AI Exploits](https://github.com/protectai/ai-exploits?tab=readme-ov-file)**

- **[From MLOps to MLOops: Exposing the Attack Surface of Machine Learning Platforms](https://jfrog.com/blog/from-mlops-to-mloops-exposing-the-attack-surface-of-machine-learning-platforms/)**

- **[Google AI Studio: LLM-Powered Data Exfiltration Hits Again! Quickly Fixed.](https://embracethered.com/blog/posts/2024/google-ai-studio-data-exfiltration-now-fixed/)**
  
- **[CSWSH Meets LLM Chatbots](https://medium.com/@r3vsh/cswsh-meets-llm-chatbots-3ab09af5ab6f)**

- **[Jailbreak of Meta AI (Llama -3.1) revealing configuration details](https://medium.com/@kiranmaraju/jailbreak-of-meta-ai-llama-3-1-revealing-configuration-details-9f0759f5006a)**

- **[Zeroday on Github Copilot](https://gccybermonks.com/posts/github/)**

- **[Sorry, ChatGPT Is Under Maintenance: Persistent Denial of Service through Prompt Injection and Memory Attacks](https://embracethered.com/blog/posts/2024/chatgpt-persistent-denial-of-service/)**

- **[Shelltorch Explained: Multiple Vulnerabilities in Pytorch Model Server (Torchserve) (CVSS 9.9, CVSS 9.8) Walkthrough](https://www.oligo.security/blog/shelltorch-explained-multiple-vulnerabilities-in-pytorch-model-server)**

- **[When Prompts Go Rogue: Analyzing a Prompt Injection Code Execution in Vanna.AI](https://jfrog.com/blog/prompt-injection-attack-code-execution-in-vanna-ai-cve-2024-5565/)**

- **[GitHub Copilot Chat: From Prompt Injection to Data Exfiltration](https://embracethered.com/blog/posts/2024/github-copilot-chat-prompt-injection-data-exfiltration/)**

- **[Dumping a Database with an AI Chatbot](https://www.synack.com/blog/dumping-a-database-with-an-ai-chatbot/)**

- **[My LLM Bug Bounty Journey on Hugging Face Hub via Protect AI](https://medium.com/@zpbrent/my-llm-bug-bounty-journey-on-hugging-face-hub-via-protect-ai-9f3a1bc72c2e)**

- **[LLM Pentest: Leveraging Agent Integration For RCE](https://www.blazeinfosec.com/post/llm-pentest-agent-hacking/)**

- **[Google AI Studio Data Exfiltration via Prompt Injection - Possible Regression and Fix](https://embracethered.com/blog/posts/2024/google-aistudio-mass-data-exfil/)**

- **[From ChatBot To SpyBot: ChatGPT Post Exploitation](https://www.imperva.com/blog/from-chatbot-to-spybot-chatgpt-post-exploitation/)**

- **[Security Flaws within ChatGPT Ecosystem Allowed Access to Accounts On Third-Party Websites and Sensitive Data](https://salt.security/blog/security-flaws-within-chatgpt-extensions-allowed-access-to-accounts-on-third-party-websites-and-sensitive-data)**

- **[New Google Gemini Vulnerability Enabling Profound Misuse](https://hiddenlayer.com/research/new-google-gemini-content-manipulation-vulns-found/#Overview)**

- **[We Hacked Google A.I. for $50,000](https://www.landh.tech/blog/20240304-google-hack-50000/)**

- **[XSS Marks the Spot: Digging Up Vulnerabilities in ChatGPT](https://www.imperva.com/blog/xss-marks-the-spot-digging-up-vulnerabilities-in-chatgpt/)**

- **[ChatGPT Account Takeover - Wildcard Web Cache Deception](https://nokline.github.io/bugbounty/2024/02/04/ChatGPT-ATO.html)**

- **[Bypass instructions to manipulate Google Bard AI (Conversational generative AI chatbot) to reveal its security vulnerability i.e. configuration file details exposure](https://medium.com/@kiranmaraju/bypass-instructions-to-manipulate-google-bard-ai-conversational-generative-ai-chatbot-to-reveal-ac23156d5eee)**

- **[AWS Fixes Data Exfiltration Attack Angle in Amazon Q for Business](https://embracethered.com/blog/posts/2024/aws-amazon-q-fixes-markdown-rendering-vulnerability/)**

- **[Hacking Google Bard - From Prompt Injection to Data Exfiltration](https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/)**

- **[Anonymised Penetration Test Report](https://handbook.volkis.com.au/assets/doc/Volkis%20-%20Anonymous%20Client%20-%20Penetration%20Test%20May%202023.pdf)**

- **[OpenAI Allowed “Unlimited” Credit on New Accounts](https://checkmarx.com/blog/openai-allowed-unlimited-credit-on-new-accounts/)**

- **[Shockwave Identifies Web Cache Deception and Account Takeover Vulnerability affecting OpenAI's ChatGPT](https://www.shockwave.cloud/blog/shockwave-works-with-openai-to-fix-critical-chatgpt-vulnerability)**

## Contributing

If you know of any other valuable resources that should be included, feel free to submit a pull request or open an issue.
-->

